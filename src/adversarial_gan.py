"""
GAN-based Adversarial Traffic Generation for DDoS Detection Evasion

This module implements a Generative Adversarial Network (GAN) framework
for generating evasive DDoS traffic patterns that can bypass detection models.

The adversarial framework includes:
- Generator: Creates synthetic traffic features that mimic benign traffic
- Discriminator: Distinguishes between real benign traffic and generated traffic
- Attack perturbation: Modifies real attack traffic to evade detection
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from typing import Dict, List, Optional, Tuple
from torch.utils.data import DataLoader, TensorDataset
from tqdm import tqdm
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class Generator(nn.Module):
    """
    Generator network for creating evasive traffic features.
    
    Takes random noise and generates traffic features that appear
    similar to benign traffic while maintaining attack characteristics.
    """
    
    def __init__(
        self,
        latent_dim: int = 100,
        output_dim: int = 76,
        hidden_dims: Tuple[int, ...] = (256, 512, 256)
    ):
        """
        Initialize the generator.
        
        Args:
            latent_dim: Dimension of input noise vector
            output_dim: Dimension of output features
            hidden_dims: Dimensions of hidden layers
        """
        super(Generator, self).__init__()
        
        self.latent_dim = latent_dim
        self.output_dim = output_dim
        
        layers = []
        in_dim = latent_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(in_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.LeakyReLU(0.2),
                nn.Dropout(0.3)
            ])
            in_dim = hidden_dim
        
        layers.append(nn.Linear(in_dim, output_dim))
        layers.append(nn.Tanh())  # Output in [-1, 1]
        
        self.model = nn.Sequential(*layers)
    
    def forward(self, z: torch.Tensor) -> torch.Tensor:
        """Generate traffic features from noise."""
        return self.model(z)


class Discriminator(nn.Module):
    """
    Discriminator network for distinguishing real vs generated traffic.
    
    Classifies whether traffic features are from real benign traffic
    or generated by the Generator.
    """
    
    def __init__(
        self,
        input_dim: int = 76,
        hidden_dims: Tuple[int, ...] = (256, 128, 64)
    ):
        """
        Initialize the discriminator.
        
        Args:
            input_dim: Dimension of input features
            hidden_dims: Dimensions of hidden layers
        """
        super(Discriminator, self).__init__()
        
        self.input_dim = input_dim
        
        layers = []
        in_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(in_dim, hidden_dim),
                nn.LeakyReLU(0.2),
                nn.Dropout(0.3)
            ])
            in_dim = hidden_dim
        
        layers.append(nn.Linear(in_dim, 1))
        layers.append(nn.Sigmoid())
        
        self.model = nn.Sequential(*layers)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Classify traffic as real (1) or fake (0)."""
        return self.model(x)


class PerturbationGenerator(nn.Module):
    """
    Network for generating adversarial perturbations.
    
    This model learns to add small perturbations to attack traffic
    to make it appear benign to the detection model while preserving
    the attack's effectiveness.
    """
    
    def __init__(
        self,
        input_dim: int = 76,
        hidden_dim: int = 128,
        epsilon: float = 0.1
    ):
        """
        Initialize the perturbation generator.
        
        Args:
            input_dim: Dimension of traffic features
            hidden_dim: Hidden layer dimension
            epsilon: Maximum perturbation magnitude
        """
        super(PerturbationGenerator, self).__init__()
        
        self.input_dim = input_dim
        self.epsilon = epsilon
        
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        self.perturbation_head = nn.Sequential(
            nn.Linear(hidden_dim, input_dim),
            nn.Tanh()  # Output in [-1, 1], scaled by epsilon
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Generate perturbation for input features."""
        encoded = self.encoder(x)
        perturbation = self.perturbation_head(encoded) * self.epsilon
        return perturbation
    
    def perturb(self, x: torch.Tensor) -> torch.Tensor:
        """Apply perturbation to input features."""
        perturbation = self.forward(x)
        return x + perturbation


class AdversarialGAN:
    """
    GAN framework for generating evasive DDoS traffic.
    
    This class orchestrates the training of Generator and Discriminator
    to create traffic patterns that can evade detection models.
    """
    
    def __init__(
        self,
        feature_dim: int = 76,
        latent_dim: int = 100,
        device: str = 'auto'
    ):
        """
        Initialize the adversarial GAN.
        
        Args:
            feature_dim: Dimension of traffic features
            latent_dim: Dimension of generator input noise
            device: Device to train on
        """
        self.feature_dim = feature_dim
        self.latent_dim = latent_dim
        
        # Set device
        if device == 'auto':
            if torch.cuda.is_available():
                self.device = torch.device('cuda')
            elif torch.backends.mps.is_available():
                self.device = torch.device('mps')
            else:
                self.device = torch.device('cpu')
        else:
            self.device = torch.device(device)
        
        # Initialize networks
        self.generator = Generator(latent_dim, feature_dim).to(self.device)
        self.discriminator = Discriminator(feature_dim).to(self.device)
        
        # Loss function
        self.criterion = nn.BCELoss()
        
        logger.info(f"Initialized AdversarialGAN on {self.device}")
    
    def train(
        self,
        benign_data: np.ndarray,
        epochs: int = 100,
        batch_size: int = 64,
        lr_g: float = 2e-4,
        lr_d: float = 2e-4,
        d_steps: int = 1,
        g_steps: int = 1
    ) -> Dict[str, List[float]]:
        """
        Train the GAN on benign traffic data.
        
        Args:
            benign_data: Benign traffic features to learn from
            epochs: Number of training epochs
            batch_size: Batch size
            lr_g: Generator learning rate
            lr_d: Discriminator learning rate
            d_steps: Discriminator updates per iteration
            g_steps: Generator updates per iteration
            
        Returns:
            Training history
        """
        # Prepare data
        dataset = TensorDataset(torch.FloatTensor(benign_data))
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        # Optimizers
        optimizer_g = optim.Adam(self.generator.parameters(), lr=lr_g, betas=(0.5, 0.999))
        optimizer_d = optim.Adam(self.discriminator.parameters(), lr=lr_d, betas=(0.5, 0.999))
        
        history = {'d_loss': [], 'g_loss': [], 'd_real': [], 'd_fake': []}
        
        logger.info(f"Training GAN for {epochs} epochs on {len(benign_data)} samples")
        
        for epoch in range(epochs):
            epoch_d_loss = 0.0
            epoch_g_loss = 0.0
            epoch_d_real = 0.0
            epoch_d_fake = 0.0
            n_batches = 0
            
            for batch_data, in dataloader:
                batch_data = batch_data.to(self.device)
                batch_size_actual = batch_data.size(0)
                
                # Labels
                real_labels = torch.ones(batch_size_actual, 1).to(self.device)
                fake_labels = torch.zeros(batch_size_actual, 1).to(self.device)
                
                # Train Discriminator
                for _ in range(d_steps):
                    optimizer_d.zero_grad()
                    
                    # Real samples
                    d_real = self.discriminator(batch_data)
                    d_loss_real = self.criterion(d_real, real_labels)
                    
                    # Fake samples
                    z = torch.randn(batch_size_actual, self.latent_dim).to(self.device)
                    fake_data = self.generator(z).detach()
                    d_fake = self.discriminator(fake_data)
                    d_loss_fake = self.criterion(d_fake, fake_labels)
                    
                    d_loss = d_loss_real + d_loss_fake
                    d_loss.backward()
                    optimizer_d.step()
                
                # Train Generator
                for _ in range(g_steps):
                    optimizer_g.zero_grad()
                    
                    z = torch.randn(batch_size_actual, self.latent_dim).to(self.device)
                    fake_data = self.generator(z)
                    d_fake = self.discriminator(fake_data)
                    
                    # Generator wants discriminator to think fake is real
                    g_loss = self.criterion(d_fake, real_labels)
                    g_loss.backward()
                    optimizer_g.step()
                
                epoch_d_loss += d_loss.item()
                epoch_g_loss += g_loss.item()
                epoch_d_real += d_real.mean().item()
                epoch_d_fake += d_fake.mean().item()
                n_batches += 1
            
            # Record history
            history['d_loss'].append(epoch_d_loss / n_batches)
            history['g_loss'].append(epoch_g_loss / n_batches)
            history['d_real'].append(epoch_d_real / n_batches)
            history['d_fake'].append(epoch_d_fake / n_batches)
            
            if (epoch + 1) % 10 == 0:
                logger.info(
                    f"Epoch {epoch+1}/{epochs} - "
                    f"D Loss: {history['d_loss'][-1]:.4f}, G Loss: {history['g_loss'][-1]:.4f}, "
                    f"D(real): {history['d_real'][-1]:.4f}, D(fake): {history['d_fake'][-1]:.4f}"
                )
        
        return history
    
    def generate(self, n_samples: int) -> np.ndarray:
        """
        Generate synthetic benign-like traffic.
        
        Args:
            n_samples: Number of samples to generate
            
        Returns:
            Generated traffic features
        """
        self.generator.eval()
        
        with torch.no_grad():
            z = torch.randn(n_samples, self.latent_dim).to(self.device)
            generated = self.generator(z)
        
        return generated.cpu().numpy()
    
    def save(self, path: str):
        """Save GAN models."""
        torch.save({
            'generator_state_dict': self.generator.state_dict(),
            'discriminator_state_dict': self.discriminator.state_dict(),
            'feature_dim': self.feature_dim,
            'latent_dim': self.latent_dim
        }, path)
        logger.info(f"GAN saved to {path}")
    
    def load(self, path: str):
        """Load GAN models."""
        checkpoint = torch.load(path, map_location=self.device)
        self.generator.load_state_dict(checkpoint['generator_state_dict'])
        self.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])
        logger.info(f"GAN loaded from {path}")


class AdversarialAttacker:
    """
    Adversarial attack framework for evading DDoS detection models.
    
    This class implements various attack strategies:
    - FGSM (Fast Gradient Sign Method)
    - PGD (Projected Gradient Descent)
    - GAN-based perturbations
    """
    
    def __init__(
        self,
        target_model: nn.Module,
        device: str = 'auto'
    ):
        """
        Initialize the adversarial attacker.
        
        Args:
            target_model: Detection model to attack
            device: Device to use
        """
        self.target_model = target_model
        
        if device == 'auto':
            if torch.cuda.is_available():
                self.device = torch.device('cuda')
            elif torch.backends.mps.is_available():
                self.device = torch.device('mps')
            else:
                self.device = torch.device('cpu')
        else:
            self.device = torch.device(device)
        
        self.target_model.to(self.device)
        self.target_model.eval()
        
        # Perturbation generator
        self.perturbation_gen = None
    
    def fgsm_attack(
        self,
        x: torch.Tensor,
        y: torch.Tensor,
        epsilon: float = 0.1,
        targeted: bool = True,
        target_class: int = 0  # Benign class
    ) -> torch.Tensor:
        """
        Fast Gradient Sign Method attack.
        
        Args:
            x: Input features
            y: True labels
            epsilon: Perturbation magnitude
            targeted: If True, minimize loss for target class
            target_class: Target class for targeted attack
            
        Returns:
            Adversarial examples
        """
        x = x.clone().detach().to(self.device).requires_grad_(True)
        y = y.clone().detach().to(self.device)
        
        self.target_model.zero_grad()
        
        outputs = self.target_model(x)
        
        if targeted:
            # Targeted attack: minimize loss for target class
            target = torch.full_like(y, target_class)
            loss = nn.CrossEntropyLoss()(outputs, target)
            loss.backward()
            
            # Move towards lower loss for target (subtract gradient)
            x_adv = x - epsilon * x.grad.sign()
        else:
            # Untargeted attack: maximize loss for true class
            loss = nn.CrossEntropyLoss()(outputs, y)
            loss.backward()
            
            # Move towards higher loss (add gradient)
            x_adv = x + epsilon * x.grad.sign()
        
        return x_adv.detach()
    
    def pgd_attack(
        self,
        x: torch.Tensor,
        y: torch.Tensor,
        epsilon: float = 0.1,
        alpha: float = 0.01,
        num_iter: int = 40,
        targeted: bool = True,
        target_class: int = 0
    ) -> torch.Tensor:
        """
        Projected Gradient Descent attack.
        
        Args:
            x: Input features
            y: True labels
            epsilon: Maximum perturbation magnitude
            alpha: Step size per iteration
            num_iter: Number of iterations
            targeted: If True, minimize loss for target class
            target_class: Target class for targeted attack
            
        Returns:
            Adversarial examples
        """
        x_orig = x.clone().detach().to(self.device)
        x_adv = x.clone().detach().to(self.device)
        y = y.clone().detach().to(self.device)
        
        for _ in range(num_iter):
            x_adv.requires_grad_(True)
            
            self.target_model.zero_grad()
            outputs = self.target_model(x_adv)
            
            if targeted:
                target = torch.full_like(y, target_class)
                loss = nn.CrossEntropyLoss()(outputs, target)
                loss.backward()
                x_adv = x_adv - alpha * x_adv.grad.sign()
            else:
                loss = nn.CrossEntropyLoss()(outputs, y)
                loss.backward()
                x_adv = x_adv + alpha * x_adv.grad.sign()
            
            # Project back to epsilon ball
            perturbation = torch.clamp(x_adv - x_orig, -epsilon, epsilon)
            x_adv = (x_orig + perturbation).detach()
        
        return x_adv
    
    def train_perturbation_generator(
        self,
        attack_data: np.ndarray,
        attack_labels: np.ndarray,
        epochs: int = 50,
        batch_size: int = 64,
        epsilon: float = 0.1,
        lr: float = 1e-3
    ) -> Dict[str, List[float]]:
        """
        Train a neural network to generate adversarial perturbations.
        
        Args:
            attack_data: Attack traffic features
            attack_labels: Attack labels
            epochs: Training epochs
            batch_size: Batch size
            epsilon: Maximum perturbation magnitude
            lr: Learning rate
            
        Returns:
            Training history
        """
        feature_dim = attack_data.shape[1]
        self.perturbation_gen = PerturbationGenerator(
            input_dim=feature_dim,
            epsilon=epsilon
        ).to(self.device)
        
        optimizer = optim.Adam(self.perturbation_gen.parameters(), lr=lr)
        
        dataset = TensorDataset(
            torch.FloatTensor(attack_data),
            torch.LongTensor(attack_labels)
        )
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        target_class = 0  # Benign
        history = {'loss': [], 'evasion_rate': []}
        
        logger.info(f"Training perturbation generator for {epochs} epochs")
        
        for epoch in range(epochs):
            epoch_loss = 0.0
            n_batches = 0
            
            self.perturbation_gen.train()
            
            for batch_x, batch_y in dataloader:
                batch_x = batch_x.to(self.device)
                
                optimizer.zero_grad()
                
                # Generate perturbed samples
                x_perturbed = self.perturbation_gen.perturb(batch_x)
                
                # Get predictions
                with torch.no_grad():
                    outputs = self.target_model(x_perturbed)
                
                # Loss: we want the model to classify perturbed attacks as benign
                target = torch.full((batch_x.size(0),), target_class, dtype=torch.long).to(self.device)
                
                # Enable gradient computation for perturbation generator
                outputs_for_loss = self.target_model(x_perturbed)
                loss = nn.CrossEntropyLoss()(outputs_for_loss, target)
                
                # Add regularization to keep perturbations small
                perturbation = self.perturbation_gen(batch_x)
                reg_loss = torch.mean(perturbation ** 2) * 0.1
                
                total_loss = loss + reg_loss
                total_loss.backward()
                optimizer.step()
                
                epoch_loss += total_loss.item()
                n_batches += 1
            
            # Evaluate evasion rate
            evasion_rate = self._evaluate_evasion(attack_data)
            
            history['loss'].append(epoch_loss / n_batches)
            history['evasion_rate'].append(evasion_rate)
            
            if (epoch + 1) % 10 == 0:
                logger.info(
                    f"Epoch {epoch+1}/{epochs} - "
                    f"Loss: {history['loss'][-1]:.4f}, Evasion Rate: {evasion_rate:.4f}"
                )
        
        return history
    
    def _evaluate_evasion(self, attack_data: np.ndarray) -> float:
        """Evaluate evasion rate on attack data."""
        self.perturbation_gen.eval()
        self.target_model.eval()
        
        x = torch.FloatTensor(attack_data).to(self.device)
        
        with torch.no_grad():
            x_perturbed = self.perturbation_gen.perturb(x)
            outputs = self.target_model(x_perturbed)
            predictions = outputs.argmax(dim=1)
        
        # Evasion rate: percentage classified as benign (class 0)
        evasion_rate = (predictions == 0).float().mean().item()
        
        return evasion_rate
    
    def generate_adversarial_examples(
        self,
        attack_data: np.ndarray,
        method: str = 'pgd'
    ) -> np.ndarray:
        """
        Generate adversarial examples using the specified method.
        
        Args:
            attack_data: Attack traffic features
            method: Attack method ('fgsm', 'pgd', 'learned')
            
        Returns:
            Adversarial examples
        """
        x = torch.FloatTensor(attack_data).to(self.device)
        y = torch.ones(len(attack_data), dtype=torch.long).to(self.device)  # Attack class
        
        if method == 'fgsm':
            x_adv = self.fgsm_attack(x, y)
        elif method == 'pgd':
            x_adv = self.pgd_attack(x, y)
        elif method == 'learned':
            if self.perturbation_gen is None:
                raise ValueError("Perturbation generator not trained. Call train_perturbation_generator first.")
            self.perturbation_gen.eval()
            with torch.no_grad():
                x_adv = self.perturbation_gen.perturb(x)
        else:
            raise ValueError(f"Unknown method: {method}")
        
        return x_adv.cpu().numpy()


def evaluate_evasion_success(
    detector: nn.Module,
    original_attacks: np.ndarray,
    adversarial_attacks: np.ndarray,
    device: str = 'cpu'
) -> Dict[str, float]:
    """
    Evaluate the success of adversarial evasion.
    
    Args:
        detector: Detection model
        original_attacks: Original attack traffic
        adversarial_attacks: Adversarial attack traffic
        device: Device to use
        
    Returns:
        Evasion metrics
    """
    detector.eval()
    device = torch.device(device)
    detector.to(device)
    
    # Evaluate original attacks
    with torch.no_grad():
        x_orig = torch.FloatTensor(original_attacks).to(device)
        outputs_orig = detector(x_orig)
        preds_orig = outputs_orig.argmax(dim=1).cpu().numpy()
    
    # Evaluate adversarial attacks
    with torch.no_grad():
        x_adv = torch.FloatTensor(adversarial_attacks).to(device)
        outputs_adv = detector(x_adv)
        preds_adv = outputs_adv.argmax(dim=1).cpu().numpy()
    
    # Calculate metrics
    original_detection_rate = np.mean(preds_orig == 1)  # Detected as attack
    adversarial_detection_rate = np.mean(preds_adv == 1)  # Detected as attack
    evasion_success_rate = np.mean(preds_adv == 0)  # Evaded (classified as benign)
    
    # Calculate perturbation metrics
    perturbation = adversarial_attacks - original_attacks
    l2_norm = np.linalg.norm(perturbation, axis=1).mean()
    linf_norm = np.abs(perturbation).max(axis=1).mean()
    
    metrics = {
        'original_detection_rate': original_detection_rate,
        'adversarial_detection_rate': adversarial_detection_rate,
        'evasion_success_rate': evasion_success_rate,
        'detection_rate_drop': original_detection_rate - adversarial_detection_rate,
        'mean_l2_perturbation': l2_norm,
        'mean_linf_perturbation': linf_norm
    }
    
    logger.info(f"Evasion Results:")
    logger.info(f"  Original Detection Rate: {original_detection_rate:.4f}")
    logger.info(f"  Adversarial Detection Rate: {adversarial_detection_rate:.4f}")
    logger.info(f"  Evasion Success Rate: {evasion_success_rate:.4f}")
    logger.info(f"  Mean L2 Perturbation: {l2_norm:.4f}")
    
    return metrics


if __name__ == '__main__':
    # Demo: Train GAN and generate adversarial examples
    print("Adversarial Traffic Generation Demo")
    print("=" * 50)
    
    from data_loader import create_synthetic_ddos_data
    from models import create_model
    
    # Create synthetic data
    X, y = create_synthetic_ddos_data(n_samples=5000)
    benign_data = X[y == 0]
    attack_data = X[y == 1]
    
    print(f"\nBenign samples: {len(benign_data)}")
    print(f"Attack samples: {len(attack_data)}")
    
    # Train a simple detector first
    print("\n" + "=" * 50)
    print("Training target detector...")
    detector = create_model('cnn', input_features=X.shape[1])
    
    # Quick training (in practice, use proper training)
    optimizer = optim.Adam(detector.parameters(), lr=1e-3)
    criterion = nn.CrossEntropyLoss()
    
    for epoch in range(5):
        detector.train()
        x_batch = torch.FloatTensor(X)
        y_batch = torch.LongTensor(y)
        
        if x_batch.dim() == 2:
            x_batch = x_batch.unsqueeze(1)
        
        optimizer.zero_grad()
        outputs = detector(x_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()
        
        print(f"Epoch {epoch+1}/5, Loss: {loss.item():.4f}")
    
    # Train GAN
    print("\n" + "=" * 50)
    print("Training GAN on benign traffic...")
    gan = AdversarialGAN(feature_dim=X.shape[1])
    gan.train(benign_data, epochs=20, batch_size=32)
    
    # Generate synthetic benign-like traffic
    print("\nGenerating synthetic traffic...")
    synthetic_traffic = gan.generate(100)
    print(f"Generated traffic shape: {synthetic_traffic.shape}")
    
    # Create adversarial attacker
    print("\n" + "=" * 50)
    print("Creating adversarial examples...")
    attacker = AdversarialAttacker(detector)
    
    # Generate adversarial examples using different methods
    attack_subset = attack_data[:200]
    
    print("\nFGSM Attack:")
    adv_fgsm = attacker.generate_adversarial_examples(attack_subset, method='fgsm')
    metrics_fgsm = evaluate_evasion_success(detector, attack_subset, adv_fgsm)
    
    print("\nPGD Attack:")
    adv_pgd = attacker.generate_adversarial_examples(attack_subset, method='pgd')
    metrics_pgd = evaluate_evasion_success(detector, attack_subset, adv_pgd)
    
    # Train learned perturbation generator
    print("\n" + "=" * 50)
    print("Training learned perturbation generator...")
    history = attacker.train_perturbation_generator(
        attack_data, np.ones(len(attack_data)), epochs=20
    )
    
    print("\nLearned Perturbation Attack:")
    adv_learned = attacker.generate_adversarial_examples(attack_subset, method='learned')
    metrics_learned = evaluate_evasion_success(detector, attack_subset, adv_learned)
